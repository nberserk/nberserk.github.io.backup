#+STARTUP: showall indent
#+STARTUP: hidestars
#+BEGIN_HTML
---
layout: post
title: parquet + avro + sparK
tags: parquet
date: 2015-12-09
---
#+END_HTML

데이터 분석을 위해 파일을 저장해야 할 필요가 있었다. 처음에는 csv파일 형식으로 저장을 했는데, 시간이 지남에 따라서 새로운 컬럼이 생기는 요구사항이 생겼다. 이런 경우 csv는 어떤 정보가 몇번째 컬럼에 있는지를 기술하지 않기 때문에 또 다른 파일에 컬럼 정보를 기록하고 데이터 타입등도 기술을 해줘야 하는 불편함이 생긴다. 
언뜻 parquet이 그런 일을 하는것이라 어렴풋이 알고 있었기 때문에 이번에 parquet을 적용해 볼겸 조사를 해봤다.
   
특징들..
- 압축 지원. 50% 정도 세이브 할 수 있다고 함. 스토리지 비용이 반이라는 얘기.
- 여러가지 serialize framework 지원 (Avro, Thrift, protocol buffer)

* column based
columnar storage다. 이렇게 접근한 이유는 크게 2가지.

보통 쿼리를 할때 모든 컬럼의 정보가 다 필요한 경우는 많지 않다. row based 라면 전체 열을 다 읽어야 쿼리를 수행할 수 있지만, parquet는 필요한 컬럼만 로드하면 된다. 여기서 속도 향샹이 생긴다.

그리고 컬럼끼리 모아서 압축을 하면 압축률이 더 좋아진다. timestamp를 가지는 컬럼이라고 생각하면 델터 인코딩 방식으로 압축을 하면 좋을 것이고, 각 컬럼의 특징이 살아 있으니 더 유리하다.
#+BEGIN_HTML
<iframe src="//www.slideshare.net/slideshow/embed_code/key/jKNTsYfuHHgao?startSlide=33" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>
#+END_HTML
2G의 원본 csv를 SNAPPY압축을 이용해서 저장하니 1G로 50%정도나 줄어들었다.

그리고 query performance도 일반 텍스트에 비해서 2배 정도 성능이 좋다고 한다.
#+BEGIN_HTML
<iframe src="//www.slideshare.net/slideshow/embed_code/key/jKNTsYfuHHgao?startSlide=34" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>
#+END_HTML

* 단점
마냥 좋기만 한것은 아니다. 분석용으로 최고의 파일 형식이라고 한 것은 데이터의 업데이트가 없다는 뜻이다. 즉, 지속적으로 값이 바뀌거나 지워진다고 하면 parquet을 이용하는 것은 자살행위다. 그럴때는 HBase 같은것을 써야하겠다.

* 환경 설정
spark + parquet + avro 를 사용하려면 다음과 같은 디펜던시가 필요하다. 
#+BEGIN_SRC xml
  <properties>
    <scala.version>2.11.4</scala.version>
    <maven.compiler.source>1.7</maven.compiler.source>
    <maven.compiler.target>1.7</maven.compiler.target>
    <parquet.version>1.8.1</parquet.version>
    <avro.version>1.7.7</avro.version>
  </properties>

  <dependencies>
    <dependency>
      <groupId>org.apache.parquet</groupId>
      <artifactId>parquet-common</artifactId>
      <version>${parquet.version}</version>
    </dependency>
    <dependency>
      <groupId>org.apache.parquet</groupId>
      <artifactId>parquet-encoding</artifactId>
      <version>${parquet.version}</version>
    </dependency>
    <dependency>
      <groupId>org.apache.parquet</groupId>
      <artifactId>parquet-column</artifactId>
      <version>${parquet.version}</version>
    </dependency>
    <dependency>
      <groupId>org.apache.parquet</groupId>
      <artifactId>parquet-hadoop</artifactId>
      <version>${parquet.version}</version>
    </dependency>

    <dependency>
      <groupId>org.apache.parquet</groupId>
      <artifactId>parquet-avro</artifactId>
      <version>${parquet.version}</version>
    </dependency>

    <dependency>
      <groupId>org.apache.hadoop</groupId>
      <artifactId>hadoop-client</artifactId>
      <version>1.1.0</version>
      <scope>provided</scope>
    </dependency>
  </dependencies>

  <dependency>
    <groupId>org.apache.avro</groupId>
    <artifactId>avro</artifactId>
    <version>${avro.version}</version>
  </dependency>

#+END_SRC

* avro schema define
자세한 사항은 [[http://avro.apache.org/docs/1.7.7/spec.html#schemas][avro spec ]]을 보면 되고, 아래처럼 정의 하면 된다. 이 파일을 avro-tools.jar를 이용하면 POJO class를 만들 수 있고 이 파일을 이용하면 프로그래밍이 조금더 이뻐질 수 있다. 아래 read/save 예제에서 User class를 사용하는데 이것이 스키마를 바탕으로 생성된 클래스이다. 필수는 아니고 POJO class가 없을때는 GenericRecord를 사용할 수 도 있다.
 
#+BEGIN_SRC json
  {
      "namespace": "com.nberserk.example.avro",
      "type": "record",
      "name": "User",
      "fields": [
          {"name": "id", "type": "string"},
          {"name": "age",  "type": "int"},
          {"name": "weight", "type":"float"}
      ]
  }
#+END_SRC
* parquet save/read in java
#+BEGIN_SRC java
  Schema schema = new Schema.Parser().parse(new File("src/test/avro/user.avro"));        
  File tmp = new File("test.parquet");
  Path path = new Path(tmp.getPath());        

  ParquetWriter<GenericRecord> writer = AvroParquetWriter
      .<GenericRecord>builder(path)
      .withSchema(schema)
      .withCompressionCodec(CompressionCodecName.SNAPPY)                
      .build();

  // Write a record with GenericRecord
  GenericRecord r = new GenericData.Record(schema);
  r.put("uid", "darren");
  r.put("age", 22);
  r.put("weight", 70.0);
  writer.write(r);
  writer.close();

  /*
  // this code use
  ParquetWriter<User> writer = AvroParquetWriter.<User>builder(path)
  .withCompressionCodec(CompressionCodecName.SNAPPY)
  .withSchema(schema)
  .build();

  User p = new Profile();
  p.setId("darren");
  p.setAge(22);
  p.setWeight(70.0);
  writer.write(p);
  writer.close();
  ,*/
  Configuration conf = new Configuration();
  AvroReadSupport.setAvroReadSchema(conf, Profile.SCHEMA$);
  ParquetReader<Profile> reader = AvroParquetReader.<Profile>builder(path)
      .withConf(conf)
      .build();
  Profile p1 = reader.read();

  assertEquals("darren", p1.getUid().toString());
  assertEquals(22, p1.getAge());
  assertEquals(77.0, p1.getWeight());


#+END_SRC
* parquet load 
여러개의 parquet 파일을 로드할때에 스키마가 서로 다르면 교집합(공통으로 가지고 있는 컬럼)만 로드 된다. 
#+BEGIN_SRC scala
  val p = sqlContext.parquetFile("s3://test.parquet")
  val multipleParquet = sqlContext.parquetFile("s3://p1", "s3://p2")
#+END_SRC


* revision history
- 2015/12/10 initial draft
